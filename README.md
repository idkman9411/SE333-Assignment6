[![playwright Tests](https://github.com/idkman9411/SE333-Assignment6/actions/workflows/playwright.yml/badge.svg)](https://github.com/idkman9411/SE333-Assignment6/actions/workflows/playwright.yml)




For the manual UI testing, I found the process to be kinda fun. It was also easy to do. Even without the instructions, it was intuitive on what I needed to do, as it is incredibly similar on how a person normal navigates a website. Having the assert text button made asserting easy, and the pop-up that asks if it grabbed the correct thing was very helpful for both ensure you did actually click and for clarify what exactly you are asserting.

Despite the ease of just browsing a website, there were some problems and critics I had. For one, If you make a mistake while recording, you either have to make a note on where you made a mistake to edit later or restart the recording. Another thing is its unclear when you should asset text vs assert visibility. The boundary's for assert value is very small and missable, taking me multiple tries to even realise that the option even worked at all. Another problem that happens when manually recording is the different boundary's between the input vs label in html. Aka clicking the checkbox vs the word in filter.It's a minor difference that can be easy to miss while recording, but causes problems later when trying to run test. 

A rather minor annoyance is that at times, the browser would randomly scroll up and down. But a big problem is that the generated code, closes after closing the browser. Not only was this annoying to find out the first time using codegen, causing me to have redo the recording, it also means you have to manually type out context.close.  As someone using playwright for the first time, it's weird that codegen doesn't do this automatically, as closing the context is important for saving videos and other methods to work properly.

For the LLM UI testing, it was not as difficult, but it is harder to quantify its quality to the manual testing. Because I did it second, and did not realize I needed to specify not to do it, the chat agent read from the traditional test to influence its  own code. It was a bit annoying that I couldn't just do a straight forward copy and paste because of the outdated prices. This is me thing, but it was a core of having to carefully type the instructions compared to navigating a website It did work on the first attempt, but again I would have to see how it would compare without having another file to compare. Something that was nice is that the LLM did add organization and comments, that you would have to do after the fact for the manual testing. 

Something that caused problems with both is a known problem with playwright regarding .click() not always working properly. Attempts to fix the flakiness, both manually and with LLM creating helper functions, only caused the clicks to stop working entirely and was a major point of frustration. When adding waitFors, playwright would timeout. Even the LLM helper function, which loops and tries to press the button multiple times, then resorts to a javascript click, just caused timeouts. When trying to use page.navigate(), it worked for proceed to checkout, but would throw errors for "continue" in contact information. I commented out the extra measures and just used regular click, as that is the method with the most success.